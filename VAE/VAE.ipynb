{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "import keras_tuner as kt \n",
    "from keras_tuner import HyperParameters as hp \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.loadtxt('Processed_Files/X_MinMaxScaler.csv', delimiter = ',')\n",
    "y = np.loadtxt('Processed_Files/y_LabelEncoder.csv', delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a train and test split\n",
    "globals()['train_X'], test_X, train_y, test_y = train_test_split(X, y, test_size = 0.2, random_state = 42, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Defining the function that builds the VAE + its hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def tune_VAE(hp):\n",
    "\n",
    "    ## DEFINING THE HYPERPARAMETERS TO BE TUNED\n",
    "\n",
    "    # Number of hiddewn layers\n",
    "    n_hidden = hp.Int('Hidden_Layers', min_value = 3, max_value = 7)\n",
    "    # Drop between each layer, which will define the size of the subsequent layer\n",
    "    layers_drop = []\n",
    "    for i in range(n_hidden):\n",
    "        layers_drop.append(hp.Float(f\"drop_{i}-{i+1}\", min_value = 1.2, max_value = 1.8))\n",
    "    # Layer dimensions, which depend on drop between layers\n",
    "    layers_dims = []\n",
    "    for i in range(n_hidden):\n",
    "        if i == 0:      # first layer\n",
    "            layers_dims.append(int(globals()['train_X'].shape[1]/layers_drop[i]))\n",
    "        else:\n",
    "            layers_dims.append(int(layers_dims[i-1]/layers_drop[i]))\n",
    "    # Activation function - https://keras.io/2.15/api/layers/activations/\n",
    "    activation_function = hp.Choice('Activation_Function', values = ['relu', 'sigmoid', 'softmax', 'softplus', 'softsign', 'tanh', 'selu', 'elu'])\n",
    "    # # Optimizer - https://keras.io/api/optimizers/\n",
    "    # optimizer = hp.Choice('Optimizer', values = ['SGD', 'RMSprop', 'Adam', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl'])\n",
    "    # Batch sizes\n",
    "    globals()['batch_size'] = hp.Choice('Batch_Size', values = [16, 32, 48, 64])\n",
    "    # batch_size = hp.Choice('Batch_Size', values = [16, 32, 48, 64])\n",
    "    # Learning rates\n",
    "    learning_rate = hp.Choice('Learning_Rate', values = [0.1, 0.01, 0.001, 0.0001, 0.00001])\n",
    "    # # Cost function weight\n",
    "    # weight = hp.Float('Reconstruction_Weight', min_value = 0.1, max_value = 0.9)\n",
    "\n",
    "\n",
    "    ## DEFINE THE SAMPLING FUNCTION FOR THE LATENT SPACE SAMPLE GENERATION\n",
    "    def sampling(args):\n",
    "        z_mean, z_log_sigma, latent_dim = args\n",
    "        epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0., stddev=1.)\n",
    "        return z_mean + K.exp(z_log_sigma) * epsilon\n",
    "\n",
    "\n",
    "    ## BUILDING THE VAE MODEL\n",
    "\n",
    "    # Initialiser function\n",
    "    initializer = tf.keras.initializers.GlorotNormal(seed = 15)\n",
    "\n",
    "    # Defining the input\n",
    "    input = tf.keras.Input(shape = (globals()['train_X'].shape[1], ), name = 'Input_Layer')\n",
    "    x = input\n",
    "\n",
    "    # Defining the encoder structure\n",
    "    for i in range(n_hidden-1):\n",
    "        x = tf.keras.layers.Dense(layers_dims[i], activation = activation_function, kernel_initializer = initializer, name = f'Encoder_{i+1}')(x)\n",
    "    # Defining the last hidden layer -> latent space\n",
    "    z_mean = tf.keras.layers.Dense(layers_dims[-1], name = 'Z_mean')(x)\n",
    "    z_log_sigma = tf.keras.layers.Dense(layers_dims[-1], name = 'Z_Log_Sigma')(x)\n",
    "    z = tf.keras.layers.Lambda(sampling, name = 'Z_Sampling_Layer')([z_mean, z_log_sigma, layers_dims[-1]])\n",
    "\n",
    "    # Building the decoder\n",
    "    latent_inputs = tf.keras.Input(shape = (layers_dims[-1], ), name = 'Input_Z_Sampling')\n",
    "    x = latent_inputs\n",
    "    # Decoder layers\n",
    "    for i in range(len(layers_dims)-1, 0, -1):\n",
    "        x = tf.keras.layers.Dense(layers_dims[i], activation = activation_function, kernel_initializer = initializer, name = f'Decoder_{len(layers_dims)-i}')(x)\n",
    "    # Defining the last hidden layer -> output\n",
    "    output = tf.keras.layers.Dense(globals()['train_X'].shape[1], activation = activation_function, kernel_initializer = initializer, name = 'Decoder_Output')(x)\n",
    "\n",
    "    # # Splitting also the encoder and decoder structures\n",
    "    encoder = tf.keras.Model(input, [z_mean, z_log_sigma, z], name = 'Encoder')\n",
    "    decoder = tf.keras.Model(latent_inputs, output, name = 'Decoder')\n",
    "\n",
    "    # Defining our VAE\n",
    "    output_vae = decoder(encoder(input)[2])\n",
    "    vae = tf.keras.Model(input, output_vae, name = 'VAE')\n",
    "\n",
    "    # Calculating the losses\n",
    "    reconstruction = layers_dims[0] * tf.keras.losses.mse(input, output_vae)\n",
    "    kl = -0.5 * K.sum(1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma), axis = 1)\n",
    "\n",
    "    # Total loss function\n",
    "    # vae_loss = reconstruction*weight + kl*(1 - weight)\n",
    "    vae_loss = K.mean(reconstruction + kl)    \n",
    "    vae.add_loss(vae_loss)\n",
    "\n",
    "    # Compiling the model\n",
    "    vae.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate))\n",
    "\n",
    "    return vae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Using tuner to tune the VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 00m 19s]\n",
      "val_loss: 1.4730570316314697\n",
      "\n",
      "Best val_loss So Far: 0.7556254863739014\n",
      "Total elapsed time: 00h 02m 58s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Results summary\n",
      "Results in AutoML_Experiments\\Initial_Trial\n",
      "Showing 10 best trials\n",
      "<keras_tuner.engine.objective.Objective object at 0x0000019B1DF3FEB0>\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "Hidden_Layers: 4\n",
      "drop_0-1: 1.7894110598404538\n",
      "drop_1-2: 1.276538942828444\n",
      "drop_2-3: 1.5740210084928214\n",
      "Activation_Function: tanh\n",
      "Batch_Size: 64\n",
      "Learning_Rate: 0.01\n",
      "drop_3-4: 1.3579238577556503\n",
      "drop_4-5: 1.502892558608145\n",
      "drop_5-6: 1.387799462100131\n",
      "drop_6-7: 1.694266214675848\n",
      "Score: 0.7556254863739014\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "Hidden_Layers: 7\n",
      "drop_0-1: 1.752892779860785\n",
      "drop_1-2: 1.762303800060789\n",
      "drop_2-3: 1.3001244185052072\n",
      "Activation_Function: softsign\n",
      "Batch_Size: 64\n",
      "Learning_Rate: 0.01\n",
      "drop_3-4: 1.2499061359256132\n",
      "drop_4-5: 1.2\n",
      "drop_5-6: 1.2\n",
      "drop_6-7: 1.2\n",
      "Score: 0.7862278819084167\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "Hidden_Layers: 3\n",
      "drop_0-1: 1.5939636989108394\n",
      "drop_1-2: 1.201895502874195\n",
      "drop_2-3: 1.7713572558917545\n",
      "Activation_Function: softplus\n",
      "Batch_Size: 16\n",
      "Learning_Rate: 0.01\n",
      "drop_3-4: 1.2659332085779946\n",
      "Score: 0.8739197254180908\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "Hidden_Layers: 3\n",
      "drop_0-1: 1.5270444315948706\n",
      "drop_1-2: 1.210933251563877\n",
      "drop_2-3: 1.698714400010522\n",
      "Activation_Function: tanh\n",
      "Batch_Size: 16\n",
      "Learning_Rate: 0.01\n",
      "drop_3-4: 1.358797612509356\n",
      "drop_4-5: 1.3430605667913558\n",
      "drop_5-6: 1.5530642665067544\n",
      "drop_6-7: 1.5432121546951036\n",
      "Score: 0.8999155759811401\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "Hidden_Layers: 4\n",
      "drop_0-1: 1.3034180840418663\n",
      "drop_1-2: 1.5392374296618345\n",
      "drop_2-3: 1.6489970049921474\n",
      "Activation_Function: softplus\n",
      "Batch_Size: 64\n",
      "Learning_Rate: 0.001\n",
      "drop_3-4: 1.2\n",
      "Score: 1.046729326248169\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "Hidden_Layers: 6\n",
      "drop_0-1: 1.3294428259645674\n",
      "drop_1-2: 1.2239031030459642\n",
      "drop_2-3: 1.5274743996429043\n",
      "Activation_Function: selu\n",
      "Batch_Size: 16\n",
      "Learning_Rate: 0.01\n",
      "drop_3-4: 1.4230952869817033\n",
      "drop_4-5: 1.490817840497953\n",
      "drop_5-6: 1.4223749578503668\n",
      "drop_6-7: 1.4453574115305503\n",
      "Score: 1.0498417615890503\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "Hidden_Layers: 6\n",
      "drop_0-1: 1.4375084230588162\n",
      "drop_1-2: 1.6488079582222515\n",
      "drop_2-3: 1.412597927090764\n",
      "Activation_Function: relu\n",
      "Batch_Size: 32\n",
      "Learning_Rate: 0.01\n",
      "drop_3-4: 1.3990366954586657\n",
      "drop_4-5: 1.2903258407030829\n",
      "drop_5-6: 1.5227503246509813\n",
      "drop_6-7: 1.2530303238902452\n",
      "Score: 1.1594711542129517\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "Hidden_Layers: 5\n",
      "drop_0-1: 1.3428996459446616\n",
      "drop_1-2: 1.2610352487490906\n",
      "drop_2-3: 1.6360789151798827\n",
      "Activation_Function: relu\n",
      "Batch_Size: 16\n",
      "Learning_Rate: 0.1\n",
      "drop_3-4: 1.752418232896488\n",
      "drop_4-5: 1.2641529301550407\n",
      "drop_5-6: 1.5256982394169663\n",
      "drop_6-7: 1.428402766843683\n",
      "Score: 1.306188941001892\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "Hidden_Layers: 6\n",
      "drop_0-1: 1.4235974645061242\n",
      "drop_1-2: 1.5196210262834784\n",
      "drop_2-3: 1.788581950470431\n",
      "Activation_Function: elu\n",
      "Batch_Size: 64\n",
      "Learning_Rate: 0.1\n",
      "drop_3-4: 1.2398192875158225\n",
      "drop_4-5: 1.7012489521561909\n",
      "drop_5-6: 1.4659432839787176\n",
      "drop_6-7: 1.292196746296714\n",
      "Score: 1.4730570316314697\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "Hidden_Layers: 4\n",
      "drop_0-1: 1.6177320498501646\n",
      "drop_1-2: 1.611598387916543\n",
      "drop_2-3: 1.3575693312364143\n",
      "Activation_Function: elu\n",
      "Batch_Size: 32\n",
      "Learning_Rate: 1e-05\n",
      "drop_3-4: 1.6364476964089476\n",
      "drop_4-5: 1.6214048111947694\n",
      "drop_5-6: 1.3343973005533607\n",
      "drop_6-7: 1.5247898984213772\n",
      "Score: 5.838639736175537\n"
     ]
    }
   ],
   "source": [
    "tuner = kt.BayesianOptimization(tune_VAE,\n",
    "                    objective = 'val_loss',\n",
    "                    max_trials = 10, \n",
    "                    directory = 'AutoML_Experiments',\n",
    "                    project_name = 'Initial_Trial',\n",
    "                    overwrite = True\n",
    "                    )\n",
    "\n",
    "# Defining a callback that stops the search if the results aren't improving\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor = 'val_loss',\n",
    "    min_delta = 0.0001,\n",
    "    patience = 20,\n",
    "    verbose = 1, \n",
    "    mode = 'min',\n",
    "    restore_best_weights = True)\n",
    "# Defining a callback that saves our model\n",
    "cp = tf.keras.callbacks.ModelCheckpoint(filepath = 'Best_Model/best_model.h5',\n",
    "                                mode = 'min', monitor = 'val_loss', verbose = 2 , save_best_only = True)\n",
    "\n",
    "# Initializing the tuner search - that will basically iterate over a certain number of different combinations (defined in the tuner above)\n",
    "tuner.search(globals()['train_X'], globals()['train_X'], epochs = 5, batch_size = globals()['batch_size'], validation_split = 0.1, callbacks = [early_stop])\n",
    "\n",
    "# Printing a summary with the results obtained during the tuning process\n",
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Hidden_Layers': 4, 'drop_0-1': 1.7894110598404538, 'drop_1-2': 1.276538942828444, 'drop_2-3': 1.5740210084928214, 'Activation_Function': 'tanh', 'Batch_Size': 64, 'Learning_Rate': 0.01, 'drop_3-4': 1.3579238577556503, 'drop_4-5': 1.502892558608145, 'drop_5-6': 1.387799462100131, 'drop_6-7': 1.694266214675848}\n"
     ]
    }
   ],
   "source": [
    "## RETRIEVING THE BEST MODEL\n",
    "\n",
    "# Getting the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(best_hps.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"VAE\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Input_Layer (InputLayer)       [(None, 48)]         0           []                               \n",
      "                                                                                                  \n",
      " Encoder (Functional)           [(None, 8),          2274        ['Input_Layer[0][0]']            \n",
      "                                 (None, 8),                                                       \n",
      "                                 (None, 8)]                                                       \n",
      "                                                                                                  \n",
      " Decoder (Functional)           (None, 48)           1448        ['Encoder[0][2]']                \n",
      "                                                                                                  \n",
      " Encoder_1 (Dense)              (None, 26)           1274        ['Input_Layer[0][0]']            \n",
      "                                                                                                  \n",
      " Encoder_2 (Dense)              (None, 20)           540         ['Encoder_1[0][0]']              \n",
      "                                                                                                  \n",
      " Encoder_3 (Dense)              (None, 12)           252         ['Encoder_2[0][0]']              \n",
      "                                                                                                  \n",
      " Z_Log_Sigma (Dense)            (None, 8)            104         ['Encoder_3[0][0]']              \n",
      "                                                                                                  \n",
      " Z_mean (Dense)                 (None, 8)            104         ['Encoder_3[0][0]']              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TFOpLa  (None, 8)           0           ['Z_Log_Sigma[0][0]']            \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.math.square_1 (TFOpLambda)  (None, 8)            0           ['Z_mean[0][0]']                 \n",
      "                                                                                                  \n",
      " tf.convert_to_tensor_1 (TFOpLa  (None, 48)          0           ['Decoder[0][0]']                \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.cast_1 (TFOpLambda)         (None, 48)           0           ['Input_Layer[0][0]']            \n",
      "                                                                                                  \n",
      " tf.math.subtract_2 (TFOpLambda  (None, 8)           0           ['tf.__operators__.add_2[0][0]', \n",
      " )                                                                'tf.math.square_1[0][0]']       \n",
      "                                                                                                  \n",
      " tf.math.exp_1 (TFOpLambda)     (None, 8)            0           ['Z_Log_Sigma[0][0]']            \n",
      "                                                                                                  \n",
      " tf.math.squared_difference_1 (  (None, 48)          0           ['tf.convert_to_tensor_1[0][0]', \n",
      " TFOpLambda)                                                      'tf.cast_1[0][0]']              \n",
      "                                                                                                  \n",
      " tf.math.subtract_3 (TFOpLambda  (None, 8)           0           ['tf.math.subtract_2[0][0]',     \n",
      " )                                                                'tf.math.exp_1[0][0]']          \n",
      "                                                                                                  \n",
      " tf.math.reduce_mean_2 (TFOpLam  (None,)             0           ['tf.math.squared_difference_1[0]\n",
      " bda)                                                            [0]']                            \n",
      "                                                                                                  \n",
      " tf.math.reduce_sum_1 (TFOpLamb  (None,)             0           ['tf.math.subtract_3[0][0]']     \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.multiply_2 (TFOpLambda  (None,)             0           ['tf.math.reduce_mean_2[0][0]']  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_3 (TFOpLambda  (None,)             0           ['tf.math.reduce_sum_1[0][0]']   \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TFOpLa  (None,)             0           ['tf.math.multiply_2[0][0]',     \n",
      " mbda)                                                            'tf.math.multiply_3[0][0]']     \n",
      "                                                                                                  \n",
      " tf.math.reduce_mean_3 (TFOpLam  ()                  0           ['tf.__operators__.add_3[0][0]'] \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " add_loss_1 (AddLoss)           ()                   0           ['tf.math.reduce_mean_3[0][0]']  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,722\n",
      "Trainable params: 3,722\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/20\n",
      "134/135 [============================>.] - ETA: 0s - loss: 1.0849\n",
      "Epoch 1: val_loss improved from inf to 0.75953, saving model to Best_Model\\best_model.h5\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "135/135 [==============================] - 2s 10ms/step - loss: 1.0824 - val_loss: 0.7595\n",
      "Epoch 2/20\n",
      "127/135 [===========================>..] - ETA: 0s - loss: 0.7618\n",
      "Epoch 2: val_loss improved from 0.75953 to 0.75762, saving model to Best_Model\\best_model.h5\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "135/135 [==============================] - 1s 6ms/step - loss: 0.7612 - val_loss: 0.7576\n",
      "Epoch 3/20\n",
      "134/135 [============================>.] - ETA: 0s - loss: 0.7581\n",
      "Epoch 3: val_loss improved from 0.75762 to 0.75651, saving model to Best_Model\\best_model.h5\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "135/135 [==============================] - 1s 6ms/step - loss: 0.7581 - val_loss: 0.7565\n",
      "Epoch 4/20\n",
      "135/135 [==============================] - ETA: 0s - loss: 0.7561\n",
      "Epoch 4: val_loss did not improve from 0.75651\n",
      "135/135 [==============================] - 1s 6ms/step - loss: 0.7561 - val_loss: 0.7634\n",
      "Epoch 5/20\n",
      "135/135 [==============================] - ETA: 0s - loss: 0.7572\n",
      "Epoch 5: val_loss did not improve from 0.75651\n",
      "135/135 [==============================] - 1s 6ms/step - loss: 0.7572 - val_loss: 0.7570\n",
      "Epoch 6/20\n",
      "128/135 [===========================>..] - ETA: 0s - loss: 0.7559\n",
      "Epoch 6: val_loss did not improve from 0.75651\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.7558 - val_loss: 0.7588\n",
      "Epoch 7/20\n",
      "126/135 [===========================>..] - ETA: 0s - loss: 0.7544\n",
      "Epoch 7: val_loss improved from 0.75651 to 0.75419, saving model to Best_Model\\best_model.h5\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "135/135 [==============================] - 1s 6ms/step - loss: 0.7549 - val_loss: 0.7542\n",
      "Epoch 8/20\n",
      "134/135 [============================>.] - ETA: 0s - loss: 0.7549\n",
      "Epoch 8: val_loss did not improve from 0.75419\n",
      "135/135 [==============================] - 1s 6ms/step - loss: 0.7551 - val_loss: 0.7554\n",
      "Epoch 9/20\n",
      "134/135 [============================>.] - ETA: 0s - loss: 0.7535\n",
      "Epoch 9: val_loss did not improve from 0.75419\n",
      "135/135 [==============================] - 1s 6ms/step - loss: 0.7537 - val_loss: 0.7563\n",
      "Epoch 10/20\n",
      "127/135 [===========================>..] - ETA: 0s - loss: 0.7551\n",
      "Epoch 10: val_loss did not improve from 0.75419\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.7543 - val_loss: 0.7604\n",
      "Epoch 11/20\n",
      "129/135 [===========================>..] - ETA: 0s - loss: 0.7544\n",
      "Epoch 11: val_loss did not improve from 0.75419\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.7541 - val_loss: 0.7558\n",
      "Epoch 12/20\n",
      "126/135 [===========================>..] - ETA: 0s - loss: 0.7550\n",
      "Epoch 12: val_loss did not improve from 0.75419\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.7545 - val_loss: 0.7564\n",
      "Epoch 13/20\n",
      "130/135 [===========================>..] - ETA: 0s - loss: 0.7533\n",
      "Epoch 13: val_loss did not improve from 0.75419\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.7534 - val_loss: 0.7557\n",
      "Epoch 14/20\n",
      "128/135 [===========================>..] - ETA: 0s - loss: 0.7530\n",
      "Epoch 14: val_loss did not improve from 0.75419\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.7530 - val_loss: 0.7551\n",
      "Epoch 15/20\n",
      "132/135 [============================>.] - ETA: 0s - loss: 0.7533\n",
      "Epoch 15: val_loss improved from 0.75419 to 0.75418, saving model to Best_Model\\best_model.h5\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n",
      "135/135 [==============================] - 1s 6ms/step - loss: 0.7534 - val_loss: 0.7542\n",
      "Epoch 16/20\n",
      "130/135 [===========================>..] - ETA: 0s - loss: 0.7531\n",
      "Epoch 16: val_loss did not improve from 0.75418\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.7531 - val_loss: 0.7578\n",
      "Epoch 17/20\n",
      "130/135 [===========================>..] - ETA: 0s - loss: 0.7538\n",
      "Epoch 17: val_loss did not improve from 0.75418\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.7534 - val_loss: 0.7545\n",
      "Epoch 18/20\n",
      "131/135 [============================>.] - ETA: 0s - loss: 0.7532\n",
      "Epoch 18: val_loss did not improve from 0.75418\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.7531 - val_loss: 0.7546\n",
      "Epoch 19/20\n",
      "132/135 [============================>.] - ETA: 0s - loss: 0.7533\n",
      "Epoch 19: val_loss did not improve from 0.75418\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 0.7531 - val_loss: 0.7551\n",
      "Epoch 20/20\n",
      "133/135 [============================>.] - ETA: 0s - loss: 0.7542\n",
      "Epoch 20: val_loss did not improve from 0.75418\n",
      "135/135 [==============================] - 1s 6ms/step - loss: 0.7537 - val_loss: 0.7578\n"
     ]
    }
   ],
   "source": [
    "# Retrieving the best model\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "model.summary()\n",
    "# Final fitting of the model\n",
    "history = model.fit(globals()['train_X'], globals()['train_X'], epochs = 20, batch_size = best_hps.values['Batch_Size'], validation_split = 0.1, callbacks = [early_stop, cp]).history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Input_Layer (InputLayer)       [(None, 48)]         0           []                               \n",
      "                                                                                                  \n",
      " Encoder_1 (Dense)              (None, 26)           1274        ['Input_Layer[0][0]']            \n",
      "                                                                                                  \n",
      " Encoder_2 (Dense)              (None, 20)           540         ['Encoder_1[0][0]']              \n",
      "                                                                                                  \n",
      " Encoder_3 (Dense)              (None, 12)           252         ['Encoder_2[0][0]']              \n",
      "                                                                                                  \n",
      " Z_mean (Dense)                 (None, 8)            104         ['Encoder_3[0][0]']              \n",
      "                                                                                                  \n",
      " Z_Log_Sigma (Dense)            (None, 8)            104         ['Encoder_3[0][0]']              \n",
      "                                                                                                  \n",
      " Z_Sampling_Layer (Lambda)      (None, 8)            0           ['Z_mean[0][0]',                 \n",
      "                                                                  'Z_Log_Sigma[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,274\n",
      "Trainable params: 2,274\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# encoder = tf.keras.Model(model.input, [model.get_layer('Z_mean'), model.get_layer('Z_Log_Sigma'), model.get_layer('tf.__operators__.add_2').output], name = 'Encoder')\n",
    "encoder = tf.keras.Model(model.input, model.layers[1].output, name = 'Encoder')\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Proceeding with Dimensionality Reduction study and comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - 1s 2ms/step\n",
      "75/75 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# Getting the encoder reduced data \n",
    "vae_reduced_train = encoder.predict(globals()['train_X'])\n",
    "vae_reduced_test = encoder.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I only want the z_mean layer, so I would want only the last output of the encoder\n",
    "vae_reduced_train = vae_reduced_train[0]\n",
    "vae_reduced_test = vae_reduced_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all the classifiers to be used\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Importing metrics\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading each classifier (with their default hyperparameters)\n",
    "svm = SVC()\n",
    "rf = RandomForestClassifier()\n",
    "gb = GradientBoostingClassifier()\n",
    "nb = GaussianNB()\n",
    "log_reg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Fitting and applying all classifiers to the original sized dataset\n",
    "svm.fit(globals()['train_X'], train_y)\n",
    "rf.fit(globals()['train_X'], train_y)\n",
    "gb.fit(globals()['train_X'], train_y)\n",
    "nb.fit(globals()['train_X'], train_y)\n",
    "log_reg.fit(globals()['train_X'], train_y)\n",
    "\n",
    "# Predicting the results of the test set\n",
    "y_pred_svm = svm.predict(test_X)\n",
    "y_pred_rf = rf.predict(test_X)\n",
    "y_pred_gb = gb.predict(test_X)\n",
    "y_pred_nb = nb.predict(test_X)\n",
    "y_pred_log_reg = log_reg.predict(test_X)\n",
    "\n",
    "# Calculating the metrics for each classifier\n",
    "f1_svm = f1_score(test_y, y_pred_svm, average = 'weighted')\n",
    "f1_rf = f1_score(test_y, y_pred_rf, average = 'weighted')\n",
    "f1_gb = f1_score(test_y, y_pred_gb, average = 'weighted')\n",
    "f1_nb = f1_score(test_y, y_pred_nb, average = 'weighted')\n",
    "f1_log_reg = f1_score(test_y, y_pred_log_reg, average = 'weighted')\n",
    "\n",
    "# Storing the metrics under a dataframe\n",
    "metrics = pd.DataFrame(columns = ['N_vars', 'SVM', 'RF', 'GB', 'NB', 'LogReg'])\n",
    "metrics.loc[0] = ['All', f1_svm, f1_rf, f1_gb, f1_nb, f1_log_reg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting and applying all classifiers to the reduced dataset\n",
    "svm.fit(vae_reduced_train, train_y)\n",
    "rf.fit(vae_reduced_train, train_y)\n",
    "gb.fit(vae_reduced_train, train_y)\n",
    "nb.fit(vae_reduced_train, train_y)\n",
    "log_reg.fit(vae_reduced_train, train_y)\n",
    "\n",
    "# Predicting the results of the test set\n",
    "y_pred_svm_vae = svm.predict(vae_reduced_test)\n",
    "y_pred_rf_vae = rf.predict(vae_reduced_test)\n",
    "y_pred_gb_vae = gb.predict(vae_reduced_test)\n",
    "y_pred_nb_vae = nb.predict(vae_reduced_test)\n",
    "y_pred_log_reg_vae = log_reg.predict(vae_reduced_test)\n",
    "\n",
    "# Calculating the metrics for each classifier\n",
    "f1_svm_vae = f1_score(test_y, y_pred_svm_vae, average = 'weighted')\n",
    "f1_rf_vae = f1_score(test_y, y_pred_rf_vae, average = 'weighted')\n",
    "f1_gb_vae = f1_score(test_y, y_pred_gb_vae, average = 'weighted')\n",
    "f1_nb_vae = f1_score(test_y, y_pred_nb_vae, average = 'weighted')\n",
    "f1_log_reg_vae = f1_score(test_y, y_pred_log_reg_vae, average = 'weighted')\n",
    "\n",
    "# Storing the metrics under a dataframe\n",
    "metrics.loc[1] = [encoder.output[2][1].shape[0], f1_svm_vae, f1_rf_vae, f1_gb_vae, f1_nb_vae, f1_log_reg_vae]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N_vars</th>\n",
       "      <th>SVM</th>\n",
       "      <th>RF</th>\n",
       "      <th>GB</th>\n",
       "      <th>NB</th>\n",
       "      <th>LogReg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>All</td>\n",
       "      <td>0.978157</td>\n",
       "      <td>0.987697</td>\n",
       "      <td>0.990248</td>\n",
       "      <td>0.978652</td>\n",
       "      <td>0.983199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>0.928305</td>\n",
       "      <td>0.947710</td>\n",
       "      <td>0.934095</td>\n",
       "      <td>0.826360</td>\n",
       "      <td>0.463874</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  N_vars       SVM        RF        GB        NB    LogReg\n",
       "0    All  0.978157  0.987697  0.990248  0.978652  0.983199\n",
       "1      8  0.928305  0.947710  0.934095  0.826360  0.463874"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
