{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identical notebook to 'autoencoders.ipynb', but in this one I use the MinMaxScaler instead of StandardScaler for the autoencoder data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "from keras_tuner import HyperParameters as hp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.loadtxt('Processed_Files/X_MinMaxScaler.csv', delimiter = ',')\n",
    "y = np.loadtxt('Processed_Files/y_LabelEncoder.csv', delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a train and test split\n",
    "globals()['train_X'], test_X, train_y, test_y = train_test_split(X, y, test_size = 0.2, random_state = 42, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Defining the function that builds the autoencoder + its hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_autoencoder(hp):\n",
    "\n",
    "    ## DEFINING THE HYPERPARAMETERS TO BE TUNED\n",
    "    # # Latent space size, i.e., number of reduced dimensions\n",
    "    # latent_space = hp.Int('Latent_Dimension', min_value = 2, max_value = X.shhape[1])\n",
    "    # Number of hiddewn layers\n",
    "    n_hidden = hp.Int('Hidden_Layers', min_value = 3, max_value = 7)\n",
    "    # Drop between each layer, which will define the size of the subsequent layer\n",
    "    layers_drop = []\n",
    "    for i in range(n_hidden):\n",
    "        layers_drop.append(hp.Float(f\"drop_{i}-{i+1}\", min_value = 1.2, max_value = 1.8))\n",
    "    # Layer dimensions, which depend on drop between layers\n",
    "    layers_dims = []\n",
    "    for i in range(n_hidden):\n",
    "        if i == 0:      # first layer\n",
    "            layers_dims.append(int(globals()['train_X'].shape[1]/layers_drop[i]))\n",
    "        else:\n",
    "            layers_dims.append(int(layers_dims[i-1]/layers_drop[i]))\n",
    "    # Activation function - https://keras.io/2.15/api/layers/activations/\n",
    "    activation_function = hp.Choice('Activation_Function', values = ['relu', 'sigmoid', 'softmax', 'softplus', 'softsign', 'tanh', 'selu', 'elu'])\n",
    "    # # Optimizer - https://keras.io/api/optimizers/\n",
    "    # optimizer = hp.Choice('Optimizer', values = ['SGD', 'RMSprop', 'Adam', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl'])\n",
    "    # Batch sizes\n",
    "    globals()['batch_size'] = hp.Choice('Batch_Size', values = [16, 32, 48, 64])\n",
    "    # batch_size = hp.Choice('Batch_Size', values = [16, 32, 48, 64])\n",
    "    # Learning rates\n",
    "    learning_rate = hp.Choice('Learning_Rate', values = [0.1, 0.01, 0.001, 0.0001, 0.00001])\n",
    "\n",
    "\n",
    "    ## BUILDING THE AUTOENCODER\n",
    "\n",
    "    # Initialiser function\n",
    "    initializer = tf.keras.initializers.GlorotNormal(seed = 15)\n",
    "\n",
    "    # Defining the input\n",
    "    input = tf.keras.Input(shape = (globals()['train_X'].shape[1], ), name = 'Input_Layer')\n",
    "    x = input\n",
    "\n",
    "    # Defining the encoder structure\n",
    "    for i in range(n_hidden-1):\n",
    "        x = tf.keras.layers.Dense(layers_dims[i], activation = activation_function, kernel_initializer = initializer, name = f'Encoder_{i+1}')(x)\n",
    "    # Defining the last hidden layer -> latent space\n",
    "    x = tf.keras.layers.Dense(layers_dims[-1], activation = activation_function, kernel_initializer = initializer, name = 'Encoder_Output')(x)\n",
    "\n",
    "    # Defining that the encoder output will be equal to the decoder input, that is equal to x for now\n",
    "    encoder_output = decoder_input = x\n",
    "\n",
    "    # Defining the decoder structure\n",
    "    for i in range(len(layers_dims)-1, 0, -1):\n",
    "        x = tf.keras.layers.Dense(layers_dims[i], activation = activation_function, kernel_initializer = initializer, name = f'Decoder_{len(layers_dims)-i}')(x)\n",
    "    # Defining the last hidden layer -> output\n",
    "    output = tf.keras.layers.Dense(globals()['train_X'].shape[1], activation = activation_function, kernel_initializer = initializer, name = 'Decoder_Output')(x)\n",
    "\n",
    "    # Splitting also the encoder and decoder structures\n",
    "    encoder = tf.keras.Model(input, encoder_output, name = 'Encoder')\n",
    "    decoder = tf.keras.Model(decoder_input, output, name = 'Decoder')\n",
    "\n",
    "    # Defining our autoencoder\n",
    "    autoencoder = tf.keras.Model(input, decoder(encoder(input)), name = 'Autoencoder')\n",
    "\n",
    "    # Compiling the model\n",
    "    autoencoder.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate), loss = 'mse', metrics = [tf.keras.metrics.RootMeanSquaredError()])\n",
    "\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Using tuner to tune the autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 250 Complete [00h 00m 17s]\n",
      "val_loss: 0.00883884821087122\n",
      "\n",
      "Best val_loss So Far: 0.0013835941208526492\n",
      "Total elapsed time: 01h 18m 17s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Results summary\n",
      "Results in AutoML_Experiments\\Initial_Trial\n",
      "Showing 10 best trials\n",
      "<keras_tuner.engine.objective.Objective object at 0x0000023DCBC6D150>\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "Hidden_Layers: 3\n",
      "drop_0-1: 1.3239958845750421\n",
      "drop_1-2: 1.2\n",
      "drop_2-3: 1.4523714176020224\n",
      "Activation_Function: elu\n",
      "Batch_Size: 32\n",
      "Learning_Rate: 0.01\n",
      "drop_3-4: 1.4438585360092966\n",
      "drop_4-5: 1.2783384428661924\n",
      "drop_5-6: 1.2\n",
      "drop_6-7: 1.4400459052613928\n",
      "Score: 0.0013835941208526492\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "Hidden_Layers: 3\n",
      "drop_0-1: 1.3013252027816042\n",
      "drop_1-2: 1.403428164435384\n",
      "drop_2-3: 1.6515322097239973\n",
      "Activation_Function: elu\n",
      "Batch_Size: 48\n",
      "Learning_Rate: 0.01\n",
      "drop_3-4: 1.5595235898578412\n",
      "drop_4-5: 1.5515910489498532\n",
      "drop_5-6: 1.434441899698823\n",
      "drop_6-7: 1.4939991185858474\n",
      "Score: 0.0016921275528147817\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "Hidden_Layers: 3\n",
      "drop_0-1: 1.4088259289213465\n",
      "drop_1-2: 1.2812296480218732\n",
      "drop_2-3: 1.6335463058292916\n",
      "Activation_Function: tanh\n",
      "Batch_Size: 16\n",
      "Learning_Rate: 0.01\n",
      "drop_3-4: 1.2\n",
      "drop_4-5: 1.4566242761506099\n",
      "drop_5-6: 1.2\n",
      "drop_6-7: 1.3518425773367304\n",
      "Score: 0.0017268727533519268\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "Hidden_Layers: 3\n",
      "drop_0-1: 1.571062959478257\n",
      "drop_1-2: 1.2\n",
      "drop_2-3: 1.4595903660845915\n",
      "Activation_Function: selu\n",
      "Batch_Size: 32\n",
      "Learning_Rate: 0.01\n",
      "drop_3-4: 1.415223778820972\n",
      "drop_4-5: 1.68748320067371\n",
      "drop_5-6: 1.274590136462947\n",
      "drop_6-7: 1.394068124075623\n",
      "Score: 0.0017946023726835847\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "Hidden_Layers: 3\n",
      "drop_0-1: 1.3124801332327476\n",
      "drop_1-2: 1.3963541539459727\n",
      "drop_2-3: 1.5869927854320156\n",
      "Activation_Function: tanh\n",
      "Batch_Size: 32\n",
      "Learning_Rate: 0.01\n",
      "drop_3-4: 1.2553031080562311\n",
      "drop_4-5: 1.2487040721958322\n",
      "drop_5-6: 1.300123182855798\n",
      "drop_6-7: 1.6838592863645054\n",
      "Score: 0.0018808020977303386\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "Hidden_Layers: 3\n",
      "drop_0-1: 1.627870672130554\n",
      "drop_1-2: 1.3932565785734834\n",
      "drop_2-3: 1.2\n",
      "Activation_Function: elu\n",
      "Batch_Size: 16\n",
      "Learning_Rate: 0.01\n",
      "drop_3-4: 1.4314428380555533\n",
      "drop_4-5: 1.6098754952095218\n",
      "drop_5-6: 1.3563828126670676\n",
      "drop_6-7: 1.3146276046091323\n",
      "Score: 0.0019261855632066727\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "Hidden_Layers: 4\n",
      "drop_0-1: 1.3768996792331627\n",
      "drop_1-2: 1.2\n",
      "drop_2-3: 1.4211984575020353\n",
      "Activation_Function: elu\n",
      "Batch_Size: 16\n",
      "Learning_Rate: 0.01\n",
      "drop_3-4: 1.2\n",
      "drop_4-5: 1.2\n",
      "drop_5-6: 1.522381323272054\n",
      "drop_6-7: 1.4936616123211834\n",
      "Score: 0.0020234216935932636\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "Hidden_Layers: 3\n",
      "drop_0-1: 1.6035835781581977\n",
      "drop_1-2: 1.3866223512536806\n",
      "drop_2-3: 1.29568581228301\n",
      "Activation_Function: selu\n",
      "Batch_Size: 32\n",
      "Learning_Rate: 0.01\n",
      "drop_3-4: 1.6365706143937875\n",
      "drop_4-5: 1.8\n",
      "drop_5-6: 1.5491773903504666\n",
      "drop_6-7: 1.348610324619005\n",
      "Score: 0.0022300861310213804\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "Hidden_Layers: 3\n",
      "drop_0-1: 1.6504066285626144\n",
      "drop_1-2: 1.239445248781797\n",
      "drop_2-3: 1.379146611618698\n",
      "Activation_Function: selu\n",
      "Batch_Size: 32\n",
      "Learning_Rate: 0.01\n",
      "drop_3-4: 1.2\n",
      "drop_4-5: 1.3822241977880372\n",
      "drop_5-6: 1.2\n",
      "drop_6-7: 1.6002767556314672\n",
      "Score: 0.002250365447252989\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "Hidden_Layers: 3\n",
      "drop_0-1: 1.4691793870792567\n",
      "drop_1-2: 1.431652680552466\n",
      "drop_2-3: 1.8\n",
      "Activation_Function: elu\n",
      "Batch_Size: 16\n",
      "Learning_Rate: 0.01\n",
      "drop_3-4: 1.4085920294155996\n",
      "drop_4-5: 1.8\n",
      "drop_5-6: 1.2\n",
      "drop_6-7: 1.4229848575679116\n",
      "Score: 0.0022568677086383104\n"
     ]
    }
   ],
   "source": [
    "tuner = kt.BayesianOptimization(tune_autoencoder,\n",
    "                    objective = 'val_loss',\n",
    "                    max_trials = 250 , \n",
    "                    directory = 'AutoML_Experiments',\n",
    "                    project_name = 'Initial_Trial',\n",
    "                    overwrite = True\n",
    "                    )\n",
    "\n",
    "# Defining a callback that stops the search if the results aren't improving\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor = 'val_loss',\n",
    "    min_delta = 0.0001,\n",
    "    patience = 20,\n",
    "    verbose = 1, \n",
    "    mode = 'min',\n",
    "    restore_best_weights = True)\n",
    "# Defining a callback that saves our model\n",
    "cp = tf.keras.callbacks.ModelCheckpoint(filepath = 'Best_Model/best_model.h5',\n",
    "                                mode = 'min', monitor = 'val_loss', verbose = 2 , save_best_only = True)\n",
    "\n",
    "# Initializing the tuner search - that will basically iterate over a certain number of different combinations (defined in the tuner above)\n",
    "tuner.search(globals()['train_X'], globals()['train_X'], epochs = 5, batch_size = globals()['batch_size'], validation_split = 0.1, callbacks = [early_stop])\n",
    "\n",
    "# Printing a summary with the results obtained during the tuning process\n",
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Hidden_Layers': 3, 'drop_0-1': 1.3239958845750421, 'drop_1-2': 1.2, 'drop_2-3': 1.4523714176020224, 'Activation_Function': 'elu', 'Batch_Size': 32, 'Learning_Rate': 0.01, 'drop_3-4': 1.4438585360092966, 'drop_4-5': 1.2783384428661924, 'drop_5-6': 1.2, 'drop_6-7': 1.4400459052613928}\n"
     ]
    }
   ],
   "source": [
    "## RETRIEVING THE BEST MODEL\n",
    "\n",
    "# Getting the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(best_hps.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Autoencoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input_Layer (InputLayer)    [(None, 48)]              0         \n",
      "                                                                 \n",
      " Encoder (Functional)        (None, 20)                3494      \n",
      "                                                                 \n",
      " Decoder (Functional)        (None, 48)                2538      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,032\n",
      "Trainable params: 6,032\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "261/270 [============================>.] - ETA: 0s - loss: 0.0110 - root_mean_squared_error: 0.1050\n",
      "Epoch 1: val_loss did not improve from 0.00079\n",
      "270/270 [==============================] - 2s 5ms/step - loss: 0.0108 - root_mean_squared_error: 0.1039 - val_loss: 0.0037 - val_root_mean_squared_error: 0.0607\n",
      "Epoch 2/100\n",
      "259/270 [===========================>..] - ETA: 0s - loss: 0.0027 - root_mean_squared_error: 0.0522\n",
      "Epoch 2: val_loss did not improve from 0.00079\n",
      "270/270 [==============================] - 1s 5ms/step - loss: 0.0027 - root_mean_squared_error: 0.0521 - val_loss: 0.0022 - val_root_mean_squared_error: 0.0471\n",
      "Epoch 3/100\n",
      "265/270 [============================>.] - ETA: 0s - loss: 0.0019 - root_mean_squared_error: 0.0439\n",
      "Epoch 3: val_loss did not improve from 0.00079\n",
      "270/270 [==============================] - 1s 4ms/step - loss: 0.0019 - root_mean_squared_error: 0.0439 - val_loss: 0.0018 - val_root_mean_squared_error: 0.0424\n",
      "Epoch 4/100\n",
      "262/270 [============================>.] - ETA: 0s - loss: 0.0015 - root_mean_squared_error: 0.0389\n",
      "Epoch 4: val_loss did not improve from 0.00079\n",
      "270/270 [==============================] - 1s 4ms/step - loss: 0.0015 - root_mean_squared_error: 0.0388 - val_loss: 0.0014 - val_root_mean_squared_error: 0.0368\n",
      "Epoch 5/100\n",
      "260/270 [===========================>..] - ETA: 0s - loss: 0.0013 - root_mean_squared_error: 0.0358\n",
      "Epoch 5: val_loss did not improve from 0.00079\n",
      "270/270 [==============================] - 1s 4ms/step - loss: 0.0013 - root_mean_squared_error: 0.0357 - val_loss: 0.0012 - val_root_mean_squared_error: 0.0350\n",
      "Epoch 6/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 0.0012 - root_mean_squared_error: 0.0353\n",
      "Epoch 6: val_loss did not improve from 0.00079\n",
      "270/270 [==============================] - 1s 4ms/step - loss: 0.0012 - root_mean_squared_error: 0.0353 - val_loss: 0.0015 - val_root_mean_squared_error: 0.0393\n",
      "Epoch 7/100\n",
      "257/270 [===========================>..] - ETA: 0s - loss: 0.0012 - root_mean_squared_error: 0.0349\n",
      "Epoch 7: val_loss did not improve from 0.00079\n",
      "270/270 [==============================] - 1s 4ms/step - loss: 0.0012 - root_mean_squared_error: 0.0349 - val_loss: 0.0012 - val_root_mean_squared_error: 0.0350\n",
      "Epoch 8/100\n",
      "259/270 [===========================>..] - ETA: 0s - loss: 0.0011 - root_mean_squared_error: 0.0330\n",
      "Epoch 8: val_loss did not improve from 0.00079\n",
      "270/270 [==============================] - 1s 4ms/step - loss: 0.0011 - root_mean_squared_error: 0.0330 - val_loss: 0.0011 - val_root_mean_squared_error: 0.0336\n",
      "Epoch 9/100\n",
      "261/270 [============================>.] - ETA: 0s - loss: 0.0011 - root_mean_squared_error: 0.0332\n",
      "Epoch 9: val_loss did not improve from 0.00079\n",
      "270/270 [==============================] - 1s 4ms/step - loss: 0.0011 - root_mean_squared_error: 0.0332 - val_loss: 0.0011 - val_root_mean_squared_error: 0.0326\n",
      "Epoch 10/100\n",
      "268/270 [============================>.] - ETA: 0s - loss: 9.7653e-04 - root_mean_squared_error: 0.0312\n",
      "Epoch 10: val_loss did not improve from 0.00079\n",
      "270/270 [==============================] - 1s 4ms/step - loss: 9.7640e-04 - root_mean_squared_error: 0.0312 - val_loss: 0.0010 - val_root_mean_squared_error: 0.0318\n",
      "Epoch 11/100\n",
      "258/270 [===========================>..] - ETA: 0s - loss: 9.3255e-04 - root_mean_squared_error: 0.0305\n",
      "Epoch 11: val_loss did not improve from 0.00079\n",
      "270/270 [==============================] - 1s 4ms/step - loss: 9.3215e-04 - root_mean_squared_error: 0.0305 - val_loss: 9.7586e-04 - val_root_mean_squared_error: 0.0312\n",
      "Epoch 12/100\n",
      "264/270 [============================>.] - ETA: 0s - loss: 9.1208e-04 - root_mean_squared_error: 0.0302\n",
      "Epoch 12: val_loss did not improve from 0.00079\n",
      "270/270 [==============================] - 1s 4ms/step - loss: 9.0977e-04 - root_mean_squared_error: 0.0302 - val_loss: 0.0011 - val_root_mean_squared_error: 0.0332\n",
      "Epoch 13/100\n",
      "260/270 [===========================>..] - ETA: 0s - loss: 9.4847e-04 - root_mean_squared_error: 0.0308\n",
      "Epoch 13: val_loss did not improve from 0.00079\n",
      "270/270 [==============================] - 1s 4ms/step - loss: 9.4612e-04 - root_mean_squared_error: 0.0308 - val_loss: 9.0897e-04 - val_root_mean_squared_error: 0.0301\n",
      "Epoch 14/100\n",
      "262/270 [============================>.] - ETA: 0s - loss: 9.0847e-04 - root_mean_squared_error: 0.0301\n",
      "Epoch 14: val_loss did not improve from 0.00079\n",
      "270/270 [==============================] - 1s 4ms/step - loss: 9.0770e-04 - root_mean_squared_error: 0.0301 - val_loss: 0.0012 - val_root_mean_squared_error: 0.0339\n",
      "Epoch 15/100\n",
      "261/270 [============================>.] - ETA: 0s - loss: 9.0453e-04 - root_mean_squared_error: 0.0301\n",
      "Epoch 15: val_loss did not improve from 0.00079\n",
      "270/270 [==============================] - 1s 4ms/step - loss: 9.0526e-04 - root_mean_squared_error: 0.0301 - val_loss: 0.0011 - val_root_mean_squared_error: 0.0333\n",
      "Epoch 16/100\n",
      "259/270 [===========================>..] - ETA: 0s - loss: 9.4794e-04 - root_mean_squared_error: 0.0308\n",
      "Epoch 16: val_loss did not improve from 0.00079\n",
      "270/270 [==============================] - 1s 4ms/step - loss: 9.4880e-04 - root_mean_squared_error: 0.0308 - val_loss: 8.5625e-04 - val_root_mean_squared_error: 0.0293\n",
      "Epoch 17/100\n",
      "268/270 [============================>.] - ETA: 0s - loss: 9.5723e-04 - root_mean_squared_error: 0.0309\n",
      "Epoch 17: val_loss did not improve from 0.00079\n",
      "270/270 [==============================] - 1s 4ms/step - loss: 9.5739e-04 - root_mean_squared_error: 0.0309 - val_loss: 0.0014 - val_root_mean_squared_error: 0.0372\n",
      "Epoch 18/100\n",
      "260/270 [===========================>..] - ETA: 0s - loss: 9.3387e-04 - root_mean_squared_error: 0.0306\n",
      "Epoch 18: val_loss did not improve from 0.00079\n",
      "270/270 [==============================] - 1s 4ms/step - loss: 9.3259e-04 - root_mean_squared_error: 0.0305 - val_loss: 8.7490e-04 - val_root_mean_squared_error: 0.0296\n",
      "Epoch 19/100\n",
      "257/270 [===========================>..] - ETA: 0s - loss: 7.5894e-04 - root_mean_squared_error: 0.0275\n",
      "Epoch 19: val_loss did not improve from 0.00079\n",
      "270/270 [==============================] - 1s 4ms/step - loss: 7.7489e-04 - root_mean_squared_error: 0.0278 - val_loss: 0.0015 - val_root_mean_squared_error: 0.0392\n",
      "Epoch 20/100\n",
      "267/270 [============================>.] - ETA: 0s - loss: 8.5185e-04 - root_mean_squared_error: 0.0292\n",
      "Epoch 20: val_loss did not improve from 0.00079\n",
      "270/270 [==============================] - 1s 4ms/step - loss: 8.4990e-04 - root_mean_squared_error: 0.0292 - val_loss: 8.1001e-04 - val_root_mean_squared_error: 0.0285\n",
      "Epoch 21/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 7.6906e-04 - root_mean_squared_error: 0.0277\n",
      "Epoch 21: val_loss did not improve from 0.00079\n",
      "270/270 [==============================] - 1s 4ms/step - loss: 7.6906e-04 - root_mean_squared_error: 0.0277 - val_loss: 0.0010 - val_root_mean_squared_error: 0.0319\n",
      "Epoch 22/100\n",
      "266/270 [============================>.] - ETA: 0s - loss: 8.0382e-04 - root_mean_squared_error: 0.0284\n",
      "Epoch 22: val_loss did not improve from 0.00079\n",
      "270/270 [==============================] - 1s 4ms/step - loss: 8.0457e-04 - root_mean_squared_error: 0.0284 - val_loss: 9.7046e-04 - val_root_mean_squared_error: 0.0312\n",
      "Epoch 23/100\n",
      "261/270 [============================>.] - ETA: 0s - loss: 7.9706e-04 - root_mean_squared_error: 0.0282\n",
      "Epoch 23: val_loss did not improve from 0.00079\n",
      "270/270 [==============================] - 1s 4ms/step - loss: 7.9987e-04 - root_mean_squared_error: 0.0283 - val_loss: 8.8589e-04 - val_root_mean_squared_error: 0.0298\n",
      "Epoch 24/100\n",
      "257/270 [===========================>..] - ETA: 0s - loss: 7.9471e-04 - root_mean_squared_error: 0.0282\n",
      "Epoch 24: val_loss did not improve from 0.00079\n",
      "270/270 [==============================] - 1s 4ms/step - loss: 7.9046e-04 - root_mean_squared_error: 0.0281 - val_loss: 8.0693e-04 - val_root_mean_squared_error: 0.0284\n",
      "Epoch 25/100\n",
      "260/270 [===========================>..] - ETA: 0s - loss: 7.7711e-04 - root_mean_squared_error: 0.0279\n",
      "Epoch 25: val_loss did not improve from 0.00079\n",
      "270/270 [==============================] - 1s 4ms/step - loss: 7.7735e-04 - root_mean_squared_error: 0.0279 - val_loss: 9.4637e-04 - val_root_mean_squared_error: 0.0308\n",
      "Epoch 26/100\n",
      "263/270 [============================>.] - ETA: 0s - loss: 8.1747e-04 - root_mean_squared_error: 0.0286\n",
      "Epoch 26: val_loss did not improve from 0.00079\n",
      "270/270 [==============================] - 1s 4ms/step - loss: 8.1879e-04 - root_mean_squared_error: 0.0286 - val_loss: 8.2162e-04 - val_root_mean_squared_error: 0.0287\n",
      "Epoch 27/100\n",
      "267/270 [============================>.] - ETA: 0s - loss: 8.0196e-04 - root_mean_squared_error: 0.0283\n",
      "Epoch 27: val_loss did not improve from 0.00079\n",
      "270/270 [==============================] - 1s 4ms/step - loss: 8.0029e-04 - root_mean_squared_error: 0.0283 - val_loss: 8.2186e-04 - val_root_mean_squared_error: 0.0287\n",
      "Epoch 28/100\n",
      "263/270 [============================>.] - ETA: 0s - loss: 8.6519e-04 - root_mean_squared_error: 0.0294\n",
      "Epoch 28: val_loss did not improve from 0.00079\n",
      "270/270 [==============================] - 1s 4ms/step - loss: 8.6197e-04 - root_mean_squared_error: 0.0294 - val_loss: 8.2078e-04 - val_root_mean_squared_error: 0.0286\n",
      "Epoch 29/100\n",
      "266/270 [============================>.] - ETA: 0s - loss: 7.8081e-04 - root_mean_squared_error: 0.0279\n",
      "Epoch 29: val_loss did not improve from 0.00079\n",
      "270/270 [==============================] - 1s 4ms/step - loss: 7.8054e-04 - root_mean_squared_error: 0.0279 - val_loss: 9.0451e-04 - val_root_mean_squared_error: 0.0301\n",
      "Epoch 30/100\n",
      "263/270 [============================>.] - ETA: 0s - loss: 8.0836e-04 - root_mean_squared_error: 0.0284\n",
      "Epoch 30: val_loss did not improve from 0.00079\n",
      "270/270 [==============================] - 1s 4ms/step - loss: 8.0624e-04 - root_mean_squared_error: 0.0284 - val_loss: 9.5635e-04 - val_root_mean_squared_error: 0.0309\n",
      "Epoch 31/100\n",
      "259/270 [===========================>..] - ETA: 0s - loss: 8.1954e-04 - root_mean_squared_error: 0.0286\n",
      "Epoch 31: val_loss did not improve from 0.00079\n",
      "270/270 [==============================] - 1s 4ms/step - loss: 8.2574e-04 - root_mean_squared_error: 0.0287 - val_loss: 8.9571e-04 - val_root_mean_squared_error: 0.0299\n",
      "Epoch 32/100\n",
      "257/270 [===========================>..] - ETA: 0s - loss: 8.1394e-04 - root_mean_squared_error: 0.0285\n",
      "Epoch 32: val_loss did not improve from 0.00079\n",
      "270/270 [==============================] - 1s 4ms/step - loss: 8.1105e-04 - root_mean_squared_error: 0.0285 - val_loss: 8.7827e-04 - val_root_mean_squared_error: 0.0296\n",
      "Epoch 33/100\n",
      "267/270 [============================>.] - ETA: 0s - loss: 7.8768e-04 - root_mean_squared_error: 0.0281\n",
      "Epoch 33: val_loss did not improve from 0.00079\n",
      "270/270 [==============================] - 1s 4ms/step - loss: 7.8930e-04 - root_mean_squared_error: 0.0281 - val_loss: 0.0011 - val_root_mean_squared_error: 0.0333\n",
      "Epoch 34/100\n",
      "266/270 [============================>.] - ETA: 0s - loss: 8.1447e-04 - root_mean_squared_error: 0.0285\n",
      "Epoch 34: val_loss did not improve from 0.00079\n",
      "270/270 [==============================] - 1s 4ms/step - loss: 8.1215e-04 - root_mean_squared_error: 0.0285 - val_loss: 0.0010 - val_root_mean_squared_error: 0.0323\n",
      "Epoch 35/100\n",
      "263/270 [============================>.] - ETA: 0s - loss: 8.1168e-04 - root_mean_squared_error: 0.0285\n",
      "Epoch 35: val_loss did not improve from 0.00079\n",
      "270/270 [==============================] - 1s 4ms/step - loss: 8.0747e-04 - root_mean_squared_error: 0.0284 - val_loss: 8.0597e-04 - val_root_mean_squared_error: 0.0284\n",
      "Epoch 36/100\n",
      "269/270 [============================>.] - ETA: 0s - loss: 7.6231e-04 - root_mean_squared_error: 0.0276\n",
      "Epoch 36: val_loss did not improve from 0.00079\n",
      "270/270 [==============================] - 1s 4ms/step - loss: 7.6237e-04 - root_mean_squared_error: 0.0276 - val_loss: 8.0836e-04 - val_root_mean_squared_error: 0.0284\n",
      "Epoch 37/100\n",
      "259/270 [===========================>..] - ETA: 0s - loss: 7.9953e-04 - root_mean_squared_error: 0.0283\n",
      "Epoch 37: val_loss did not improve from 0.00079\n",
      "270/270 [==============================] - 1s 5ms/step - loss: 7.9537e-04 - root_mean_squared_error: 0.0282 - val_loss: 8.4710e-04 - val_root_mean_squared_error: 0.0291\n",
      "Epoch 38/100\n",
      "263/270 [============================>.] - ETA: 0s - loss: 7.2075e-04 - root_mean_squared_error: 0.0268\n",
      "Epoch 38: val_loss did not improve from 0.00079\n",
      "270/270 [==============================] - 1s 4ms/step - loss: 7.1947e-04 - root_mean_squared_error: 0.0268 - val_loss: 8.0342e-04 - val_root_mean_squared_error: 0.0283\n",
      "Epoch 39/100\n",
      "270/270 [==============================] - ETA: 0s - loss: 7.6304e-04 - root_mean_squared_error: 0.0276\n",
      "Epoch 39: val_loss improved from 0.00079 to 0.00076, saving model to Best_Model\\best_model.h5\n",
      "270/270 [==============================] - 1s 4ms/step - loss: 7.6304e-04 - root_mean_squared_error: 0.0276 - val_loss: 7.6040e-04 - val_root_mean_squared_error: 0.0276\n",
      "Epoch 40/100\n",
      "266/270 [============================>.] - ETA: 0s - loss: 7.6360e-04 - root_mean_squared_error: 0.0276\n",
      "Epoch 40: val_loss did not improve from 0.00076\n",
      "270/270 [==============================] - 1s 4ms/step - loss: 7.6144e-04 - root_mean_squared_error: 0.0276 - val_loss: 8.4149e-04 - val_root_mean_squared_error: 0.0290\n",
      "Epoch 41/100\n",
      "269/270 [============================>.] - ETA: 0s - loss: 7.8342e-04 - root_mean_squared_error: 0.0280\n",
      "Epoch 41: val_loss did not improve from 0.00076\n",
      "270/270 [==============================] - 1s 4ms/step - loss: 7.8348e-04 - root_mean_squared_error: 0.0280 - val_loss: 8.5933e-04 - val_root_mean_squared_error: 0.0293\n",
      "Epoch 42/100\n",
      "265/270 [============================>.] - ETA: 0s - loss: 7.1798e-04 - root_mean_squared_error: 0.0268\n",
      "Epoch 42: val_loss did not improve from 0.00076\n",
      "270/270 [==============================] - 1s 4ms/step - loss: 7.1774e-04 - root_mean_squared_error: 0.0268 - val_loss: 0.0011 - val_root_mean_squared_error: 0.0334\n",
      "Epoch 43/100\n",
      "261/270 [============================>.] - ETA: 0s - loss: 8.4780e-04 - root_mean_squared_error: 0.0291\n",
      "Epoch 43: val_loss did not improve from 0.00076\n",
      "270/270 [==============================] - 1s 4ms/step - loss: 8.4377e-04 - root_mean_squared_error: 0.0290 - val_loss: 0.0010 - val_root_mean_squared_error: 0.0318\n",
      "Epoch 44/100\n",
      "258/270 [===========================>..] - ETA: 0s - loss: 7.1668e-04 - root_mean_squared_error: 0.0268Restoring model weights from the end of the best epoch: 24.\n",
      "\n",
      "Epoch 44: val_loss improved from 0.00076 to 0.00073, saving model to Best_Model\\best_model.h5\n",
      "270/270 [==============================] - 1s 5ms/step - loss: 7.1325e-04 - root_mean_squared_error: 0.0267 - val_loss: 7.3278e-04 - val_root_mean_squared_error: 0.0271\n",
      "Epoch 44: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Retrieving the best model\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "model.summary()\n",
    "# Final fitting of the model\n",
    "history = model.fit(globals()['train_X'], globals()['train_X'], epochs = 100, batch_size = best_hps.values['Batch_Size'], validation_split = 0.1, callbacks = [early_stop, cp]).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input_Layer (InputLayer)    [(None, 48)]              0         \n",
      "                                                                 \n",
      " Encoder_1 (Dense)           (None, 36)                1764      \n",
      "                                                                 \n",
      " Encoder_2 (Dense)           (None, 30)                1110      \n",
      "                                                                 \n",
      " Encoder_Output (Dense)      (None, 20)                620       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,494\n",
      "Trainable params: 3,494\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Retrieving the encoder model - what actually matters for Dimensionality Reduction\n",
    "encoder = tf.keras.Model(model.input, model.layers[-2].output)\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Proceeding with Dimensionality Reduction study and comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/300 [..............................] - ETA: 4s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - 0s 1ms/step\n",
      "75/75 [==============================] - 0s 941us/step\n"
     ]
    }
   ],
   "source": [
    "# Getting the encoder reduced data \n",
    "encoder_reduced_train = encoder.predict(globals()['train_X'])\n",
    "encoder_reduced_test = encoder.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all the classifiers to be used\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Importing metrics\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading each classifier (with their default hyperparameters)\n",
    "svm = SVC()\n",
    "rf = RandomForestClassifier()\n",
    "gb = GradientBoostingClassifier()\n",
    "nb = GaussianNB()\n",
    "log_reg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Fitting and applying all classifiers to the original sized dataset\n",
    "svm.fit(globals()['train_X'], train_y)\n",
    "rf.fit(globals()['train_X'], train_y)\n",
    "gb.fit(globals()['train_X'], train_y)\n",
    "nb.fit(globals()['train_X'], train_y)\n",
    "log_reg.fit(globals()['train_X'], train_y)\n",
    "\n",
    "# Predicting the results of the test set\n",
    "y_pred_svm = svm.predict(test_X)\n",
    "y_pred_rf = rf.predict(test_X)\n",
    "y_pred_gb = gb.predict(test_X)\n",
    "y_pred_nb = nb.predict(test_X)\n",
    "y_pred_log_reg = log_reg.predict(test_X)\n",
    "\n",
    "# Calculating the metrics for each classifier\n",
    "f1_svm = f1_score(test_y, y_pred_svm, average = 'weighted')\n",
    "f1_rf = f1_score(test_y, y_pred_rf, average = 'weighted')\n",
    "f1_gb = f1_score(test_y, y_pred_gb, average = 'weighted')\n",
    "f1_nb = f1_score(test_y, y_pred_nb, average = 'weighted')\n",
    "f1_log_reg = f1_score(test_y, y_pred_log_reg, average = 'weighted')\n",
    "\n",
    "# Storing the metrics under a dataframe\n",
    "metrics = pd.DataFrame(columns = ['N_vars', 'SVM', 'RF', 'GB', 'NB', 'LogReg'])\n",
    "metrics.loc[0] = ['All', f1_svm, f1_rf, f1_gb, f1_nb, f1_log_reg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting and applying all classifiers to the reduced dataset\n",
    "svm.fit(encoder_reduced_train, train_y)\n",
    "rf.fit(encoder_reduced_train, train_y)\n",
    "gb.fit(encoder_reduced_train, train_y)\n",
    "nb.fit(encoder_reduced_train, train_y)\n",
    "log_reg.fit(encoder_reduced_train, train_y)\n",
    "\n",
    "# Predicting the results of the test set\n",
    "y_pred_svm_encoder = svm.predict(encoder_reduced_test)\n",
    "y_pred_rf_encoder = rf.predict(encoder_reduced_test)\n",
    "y_pred_gb_encoder = gb.predict(encoder_reduced_test)\n",
    "y_pred_nb_encoder = nb.predict(encoder_reduced_test)\n",
    "y_pred_log_reg_encoder = log_reg.predict(encoder_reduced_test)\n",
    "\n",
    "# Calculating the metrics for each classifier\n",
    "f1_svm_encoder = f1_score(test_y, y_pred_svm_encoder, average = 'weighted')\n",
    "f1_rf_encoder = f1_score(test_y, y_pred_rf_encoder, average = 'weighted')\n",
    "f1_gb_encoder = f1_score(test_y, y_pred_gb_encoder, average = 'weighted')\n",
    "f1_nb_encoder = f1_score(test_y, y_pred_nb_encoder, average = 'weighted')\n",
    "f1_log_reg_encoder = f1_score(test_y, y_pred_log_reg_encoder, average = 'weighted')\n",
    "\n",
    "# Storing the metrics under a dataframe\n",
    "metrics.loc[1] = [encoder.output.shape[1], f1_svm_encoder, f1_rf_encoder, f1_gb_encoder, f1_nb_encoder, f1_log_reg_encoder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N_vars</th>\n",
       "      <th>SVM</th>\n",
       "      <th>RF</th>\n",
       "      <th>GB</th>\n",
       "      <th>NB</th>\n",
       "      <th>LogReg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>All</td>\n",
       "      <td>0.978157</td>\n",
       "      <td>0.987704</td>\n",
       "      <td>0.990669</td>\n",
       "      <td>0.978652</td>\n",
       "      <td>0.983199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.0</td>\n",
       "      <td>0.978912</td>\n",
       "      <td>0.985820</td>\n",
       "      <td>0.983163</td>\n",
       "      <td>0.972681</td>\n",
       "      <td>0.977353</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  N_vars       SVM        RF        GB        NB    LogReg\n",
       "0    All  0.978157  0.987704  0.990669  0.978652  0.983199\n",
       "1   20.0  0.978912  0.985820  0.983163  0.972681  0.977353"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
